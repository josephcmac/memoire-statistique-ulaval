\chapter{Préliminaires mathématiques}
\label{chap:math}

Ce chapitre rappelle brièvement des outils utilisés ultérieurement : la divergence de Kullback--Leibler, la géométrie de l'information (métrique de Fisher et distance de Fisher--Rao) et des repères sur la taille d'effet.

\section{Divergence de Kullback--Leibler}
Pour deux lois $P,Q$ de densités $p,q$, \citep{ay2017information}
\begin{equation}
	D_{KL}(P\|Q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx.
\end{equation}
Propriétés succinctes : $D_{KL}\ge0$ (égalité ssi $P=Q$ p.p.), asymétrie $D_{KL}(P\|Q)\neq D_{KL}(Q\|P)$, sensibilité aux queues de $Q$.

Pour des normales univariées $P=\mathcal{N}(\mu_1,\sigma_1^2)$ et $Q=\mathcal{N}(\mu_2,\sigma_2^2)$, \citep{belov2011distributions}
\begin{equation}
	D_{\mathrm{KL}}(\mu_{1},\sigma_{1};\mu_{2},\sigma_{2})
	= \ln\left(\frac{\sigma_{2}}{\sigma_{1}}\right)
	+ \frac{\sigma_{1}^{2}+(\mu_{2}-\mu_{1})^{2}}{2\sigma_{2}^{2}} - \tfrac{1}{2}.
	\label{eq:kl_normal}
\end{equation}
À l'ordre deux autour de $\theta_0$, $D_{KL}\big(p(\cdot;\theta)\|p(\cdot;\theta_0)\big)\approx\tfrac{1}{2}(\theta-\theta_0)^\top I(\theta_0)(\theta-\theta_0)$ (avec $I$ la matrice d'information de Fisher).

\section{Géométrie de l'information et distance de Fisher--Rao}
Pour une famille paramétrique $p(x;\theta)$, l'information de Fisher est \citep{ay2017information}
\begin{equation}
	g_{ij}(\theta)=\mathbb{E}_{\theta}\!\left[\partial_{i}\log p(X;\theta)\,\partial_{j}\log p(X;\theta)\right],
\end{equation}
qui définit une métrique riemannienne sur l'espace des paramètres. La distance de Fisher--Rao $d_{\mathrm{FR}}(\theta_1,\theta_2)$ est la longueur minimale d'une courbe joinant $\theta_1$ et $\theta_2$ pour cette métrique.


Pour $\mathcal{N}(\mu,\sigma^{2})$ (paramètres $(\mu,\sigma)$, $\sigma>0$),
\begin{equation}
	I(\mu,\sigma)=\begin{pmatrix} \dfrac{1}{\sigma^{2}} & 0\\[4pt] 0 & \dfrac{2}{\sigma^{2}} \end{pmatrix},
\end{equation}
montrant l'orthogonalité locale des directions translation/échelle. Pour deux normales, \citep{nielsen2023simple}
\begin{equation}
	d_{\mathrm{FR}}\big((\mu_1,\sigma_1),(\mu_2,\sigma_2)\big)
	=\sqrt{2}\,\operatorname{acosh}\!\left(1+\frac{(\mu_2-\mu_1)^2+2(\sigma_2-\sigma_1)^2}{4\sigma_1\sigma_2}\right).
	\label{eq:fr_normal}
\end{equation}

\section{Taille de l'effet}
La taille d'effet mesure l'amplitude d'une différence indépendamment de la taille d'échantillon.

\subsection*{Proportions (g de Cohen)}
Pour une proportion $P$,
\begin{equation}
	g = P - \tfrac12.
\end{equation}

Selon Jacob Cohen \citep{cohen1992power}, $g\approx 0,05$ (petit), $0,15$ (moyen), $0,25$ (grand).
