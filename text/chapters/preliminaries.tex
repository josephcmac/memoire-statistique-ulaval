\chapter{Préliminaires mathématiques}
\label{chap:math}
Ce chapitre présente brièvement les outils mathématiques utilisés dans les développements ultérieurs.

\section{Divergence de Kullback–Leibler}
Pour deux lois de probabilité $P$ et $Q$ ayant respectivement pour densités $p$ et $q$, la \emph{divergence de Kullback–Leibler} est définie par \citep{ay2017information} :
\begin{equation}
	D_{KL}(P\|Q) = \int p(x)\log\frac{p(x)}{q(x)}\,dx.
\end{equation}
Quelques propriétés fondamentales :
\begin{enumerate}
	\item $D_{KL}(P\|Q) \ge 0$ ;
	\item $D_{KL}(P\|Q) = 0$ si et seulement si $P = Q$ presque partout ;
	\item La divergence est asymétrique : $D_{KL}(P\|Q) = D_{KL}(Q\|P)$ n'est pas toujours vraie.
\end{enumerate}
Dans le cas de lois normales univariées $P = \mathcal{N}(\mu_1, \sigma_1^2)$ et $Q = \mathcal{N}(\mu_2, \sigma_2^2)$, on a \citep{belov2011distributions} :
\begin{equation}
	D_{\mathrm{KL}}(\mu_{1},\sigma_{1};\mu_{2},\sigma_{2})
	= \ln\left(\frac{\sigma_{2}}{\sigma_{1}}\right)
	+ \frac{\sigma_{1}^{2}+(\mu_{2}-\mu_{1})^{2}}{2\sigma_{2}^{2}} - \tfrac{1}{2}.
	\label{eq:kl_normal}
\end{equation}

\section{Distance de Fisher Rao}
La géométrie de l'information traite les familles de distributions de probabilité comme des variétés riemanniennes, en utilisant des outils de la géométrie différentielle pour étudier leurs propriétés \citep{ay2017information}.

\subsection{Métrique de Fisher}
Pour une famille paramétrique de densités $p(x;\theta)$, la \emph{matrice d'information de Fisher}, qui définit la métrique riemannienne, est donnée par :
\begin{equation}
	g_{ij}(\theta) = \mathbb{E}_{\theta}\!\left[\partial_{i}\log p(X;\theta)\,\partial_{j}\log p(X;\theta)\right].
\end{equation}

\subsection{Distance de Fisher–Rao}
La \emph{distance de Fisher–Rao} $d_{\mathrm{FR}}(\theta_1,\theta_2)$ est la longueur minimale d’une courbe reliant $\theta_1$ à $\theta_2$ selon la métrique de Fisher.

Pour deux lois normales univariées $\mathcal{N}(\mu_1, \sigma_1^2)$ et $\mathcal{N}(\mu_2, \sigma_2^2)$, la distance de Fisher–Rao est \citep{nielsen2023simple} :
\begin{equation}
	d_{\mathrm{FR}}\big((\mu_1,\sigma_1),(\mu_2,\sigma_2)\big)
	= \sqrt{2}\,\operatorname{acosh}\!\left(1 + \frac{(\mu_2 - \mu_1)^2 + 2(\sigma_2 - \sigma_1)^2}{4\sigma_1\sigma_2}\right).
	\label{eq:fr_normal}
\end{equation}

\section{Taille de l'effet}
La \emph{taille de l'effet} quantifie l'ampleur d'une différence indépendamment de la taille de l’échantillon. Elle est utile pour interpréter la pertinence pratique d’un résultat statistique.

\subsection{Proportions (g de Cohen)}
Pour une proportion $P$, la taille d’effet selon la statistique \emph{g de Cohen} est définie par :
\begin{equation}
	g = P - \tfrac{1}{2}.
	\label{eq:g_cohen}
\end{equation}
Selon les seuils proposés par Jacob Cohen \citep{cohen1992power}, on considère :
\begin{enumerate}
	\item $|g| \approx 0{,}05$ : effet faible ;
	\item $|g| \approx 0{,}15$ : effet moyen ;
	\item $|g| \approx 0{,}25$ : effet fort.
\end{enumerate}

\section{Bootstrapping}
Le \emph{bootstrapping} est une méthode de rééchantillonnage statistique qui permet d'estimer la variabilité d'un estimateur ou de construire des intervalles de confiance à partir d'un seul échantillon, sans supposer une forme paramétrique spécifique pour la distribution sous-jacente \citep{tibshirani1993introduction}.

L'approche non paramétrique consiste à générer de multiples échantillons bootstrap en tirant avec remplacement $n$ observations à partir de l'échantillon original de taille $n$. Pour une statistique d'intérêt $\hat{\theta}$, on calcule $\hat{\theta}^*$ pour chaque échantillon bootstrap. La distribution empirique des $\hat{\theta}^*$ approxime la distribution d'échantillonnage de $\hat{\theta}$.

Par exemple, l'écart-type bootstrap d'un estimateur est :
\begin{equation}
	\widehat{\mathrm{SE}}_{\mathrm{boot}} = \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\hat{\theta}^{*b} - \bar{\hat{\theta}}^*)^2},
\end{equation}
où $B$ est le nombre d'échantillons bootstrap, $\hat{\theta}^{*b}$ est la statistique pour le $b$-ième bootstrap, et $\bar{\hat{\theta}}^*$ est la moyenne des $\hat{\theta}^{*b}$.

Des variantes incluent le bootstrap paramétrique, où les rééchantillons sont tirés d'une distribution ajustée à l'échantillon original.

\section{Méthode des moindres carrés ordinaire}

La méthode des moindres carrés ordinaires est un outil statistique fondamental pour modéliser la relation linéaire entre une variable réponse $y$ et une ou plusieurs variables explicatives $x$. Elle sera employée dans la méthodologie pour estimer les associations entre les concentrations d'arsenic atmosphérique (transformée par le logarithme) et la prévalence des maladies respiratoires chroniques (transformée par log-cotes), stratifiées par âge et sexe. Cette section introduit le modèle, ses hypothèses, les méthodes d'estimation, ainsi que les outils de validation et de diagnostic essentiels pour en vérifier la validité des modèles linéaires.

\subsection{Modèle et hypothèses}
Considérons un échantillon de $n$ observations bivariées $(x_i, y_i)$ pour $i = 1, \dots, n$. Le modèle de régression linéaire simple postule :
\begin{equation}
	y_i = \alpha + \beta x_i + \epsilon_i,
\end{equation}
où $\alpha$ est l'ordonnée à l'origine, $\beta$ la pente (coefficient de régression), et $\epsilon_i$ les termes d'erreur, supposés indépendants et identiquement distribués selon une loi normale $\mathcal{N}(0, \sigma^2)$.

Les hypothèses principales de la méthode des moindres carrés ordinaires, avec une seule variable explicative $x$, sont :
\begin{enumerate}
	\item \emph{Linéarité} : La relation entre $x$ et $y$ est linéaire en les paramètres.
	\item \emph{Indépendance} : Les erreurs $\epsilon_i$ sont indépendantes.
	\item \emph{Homoscédasticité} : La variance des erreurs est constante, $\mathrm{Var}(\epsilon_i) = \sigma^2$.
	\item \emph{Normalité} : Les erreurs suivent une distribution normale.
\end{enumerate}
La violation de ces hypothèses peut entraîner des estimations biaisées ou inefficaces, rendant nécessaire des diagnostics rigoureux.

\subsection{Estimation des paramètres}
En vertu du théorème de Gauss–Markov \citep{monahan2008primer}, l'estimateur des moindres carrés, qui minimise la somme des carrés des résidus
\begin{equation}
	\left(\hat{\alpha}, \hat{\beta}\right) = \arg\min_{\left(\alpha, \beta \right)} \sum_{i=1}^n (y_i - \alpha - \beta x_i)^2,
\end{equation}
est le meilleur estimateur linéaire non biaisé. Les solutions explicites sont :
\begin{eqnarray}
	\hat{\beta} &=& \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}, \\
	\hat{\alpha} &=& \bar{y} - \hat{\beta} \bar{x},
\end{eqnarray}
où $\bar{x}$ et $\bar{y}$ sont les moyennes empiriques. La variance résiduelle est estimée par
\begin{equation}
	\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n \hat{\epsilon}_i^2,
\end{equation}
avec
\begin{equation}
	\hat{\epsilon}_i = y_i - \hat{\alpha} - \hat{\beta} x_i.
\end{equation}
Les métriques de qualité d'ajustement incluent le coefficient de détermination
\begin{equation}
	R^2 = 1 - \frac{\sum_{i=1}^n \hat{\epsilon}_i^2}{\sum_{i=1}^n (y_i - \bar{y})^2},
\end{equation}
qui mesure la proportion de variance expliquée par le modèle, et le $R^2$ ajusté tenant compte du nombre de paramètres :
\begin{equation}
	R^2_{\mathrm{adj}} = 1 - (1 - R^2) \frac{n-1}{n-2}.
\end{equation}
La significativité globale est évaluée par la statistique
\begin{equation}
	F = \frac{(n-2) R^2}{1 - R^2},
\end{equation}
suivant une loi de Fisher $F(1, n-2)$ sous l'hypothèse nulle $\beta = 0$. Les tests individuels pour $\alpha$ et $\beta$ reposent sur des statistiques $t$,
\begin{equation}
	t = \frac{\hat{\beta}}{\mathrm{SE}(\hat{\beta})},
\end{equation}
suivant une loi de Student $t(n-2)$, où
\begin{equation}
	\mathrm{SE}(\hat{\beta}) = \frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}}.
\end{equation}

\subsection{Validation du modèle}
La validation croisée \citep{James2021, hastie2009elements} évalue la performance prédictive. Dans la validation croisée à $k$ plis (e.g., $k=10$), l'échantillon est divisé en $k$ sous-ensembles ; le modèle est ajusté sur $k-1$ plis et testé sur le pli restant, répété $k$ fois. La racine carrée de l'erreur quadratique moyenne (RMSE) agrégée mesure l'erreur prédictive :
\begin{equation}
	\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i^{(-i)})^2},
\end{equation}
où $\hat{y}_i^{(-i)}$ est la prédiction pour l'observation $i$ exclue de l'ajustement. Cela aide à détecter le surajustement et à comparer des modèles.

\subsection{Diagnostics du modèle}
Pour valider les hypothèses, ainsi que pour détecter des points d'influence \citep{cook1982residuals}, plusieurs tests et mesures sont employés :
\begin{itemize}
	\item \emph{Linéarité et spécification} : Le test RESET\footnote{Acronyme en anglais : \emph{Regression Equation Specification Error Test}.} \citep{ramsey1969tests} vérifie si des puissances supérieures des prédictions améliorent le modèle, testant l'hypothèse nulle d'une forme linéaire adéquate.
	\item \emph{Normalité des résidus} : Le test de Shapiro-Wilk \citep{shapiro1965analysis} évalue si les résidus $\hat{\epsilon}_i$ suivent une distribution normale, avec statistique $W$ proche de 1 indiquant la normalité.
	\item \emph{Homoscédasticité} : Le test de Breusch-Pagan \citep{breusch1979simple} examine si la variance des résidus dépend des prédicteurs, via une régression auxiliaire des carrés des résidus sur $x$.
	\item \emph{Indépendance des résidus} : Le test de Durbin-Watson \citep{durbin1950testing} détecte l'autocorrélation de premier ordre, avec statistique $d \approx 2(1 - \rho)$, où $\rho$ est le coefficient d'autocorrélation ; des valeurs proches de 2 indiquent l'absence d'autocorrélation.
	\item \emph{Valeurs aberrantes} : Les résidus studentisés \citep{belsley1980regression} $|r_i| > 3$ signalent des outliers potentiels ; un test formel avec correction de Bonferroni ajuste les valeurs $p$ pour des comparaisons multiples.
	\item \emph{Points influents} : Le levier \citep{belsley1980regression}
	\begin{equation}
		h_{ii} = \frac{\partial\hat{y}_i}{\partial y_i}
	\end{equation}
	mesure l'éloignement de $x_i$ ; un seuil courant est $2 \times 2 / n = 4/n$. La distance de Cook
	\begin{equation}
		D_i = \frac { \sum_{j=1}^n \left( \widehat{y\,}_j - \widehat{y\,}_{j}^{(-i)} \right)^2 } {p \hat{\sigma}^2},
	\end{equation}
	où $p$ est le rang du modèle et $\hat{\sigma}^2$ est l'erreur quadratique moyenne,
	quantifie l'influence sur les coefficients du modèle de régression, avec seuil $4/(n-2)$. DFFITS \citep{belsley1980regression}
	\begin{equation}
		\text{DFFITS} = t_{i}^{(-i)} \sqrt{\frac{h_{ii}}{1-h_{ii}}},
	\end{equation}
	où $t_{i}^{(-i)}$ est le résidu studentisé extérieurement,
	mesure le changement standardisé dans les prédictions.
\end{itemize}

\section{Régression robuste}
La régression robuste \citep{andersen2008modern} vise à estimer les paramètres d'un modèle linéaire en minimisant l'impact des valeurs aberrantes, contrairement à la méthode des moindres carrés ordinaire qui est sensible à ces anomalies. Elle est particulièrement utile dans les contextes où les données peuvent contenir des outliers ou être distribuées selon une loi à queues épaisses \citep{taleb2020statistical}.

\subsection{Point de rupture}
Le \emph{point de rupture} \citep{rousseeuw2003robust} d'un estimateur est une mesure de sa robustesse, définie formellement comme la plus petite fraction $\epsilon \in [0,1]$ de données contaminées qui peut faire diverger l'estimateur de manière arbitraire. Pour un échantillon de taille $n$, le point de rupture fini-échantillon est $\epsilon^* = \min\{k/n\}$ où $k$ est le plus petit entier tel que, en remplaçant $k$ observations par des valeurs arbitraires choisies de manière défavorable, la norme de l'estimateur puisse devenir arbitrairement grande.

\subsection{Estimateur de Theil-Sen}
L'estimateur de Theil-Sen \citep{theil1950rank, sen1968estimates}, également connu sous le nom de méthode de la médiane unique, calcule les pentes des lignes reliant toutes les paires de points distincts dans l'échantillon. Pour un échantillon de taille $n$, cela produit $n(n-1)/2$ pentes (en supposant des valeurs $x$ distinctes). La pente du modèle est la médiane de ces valeurs. L'ordonnée à l'origine est ensuite obtenue comme la médiane des intercepts calculés pour chaque point en utilisant cette pente.
 
\subsection{Médianes répétées de Siegel}
 La méthode des médianes répétées de Siegel \citep{siegel1982robust} est une extension plus efficace de l'estimateur de Theil-Sen pour la régression linéaire simple. Elle vise à estimer la pente et l'ordonnée à l'origine d'un modèle linéaire en minimisant l'impact des valeurs aberrantes, avec un point de rupture asymptotique de 50 \% \citep{rousseeuw2003robust}, ce qui signifie qu'elle peut tolérer jusqu'à la moitié des données contaminées sans diverger.
 
 Considérons un échantillon de données bivariées \((x_i, y_i)\) pour \(i = 1, \dots, n\), où les \(x_i\) sont supposés distincts. L'estimation de la pente \(\beta\) se déroule en deux étapes imbriquées :
 
 1. Pour chaque point \(i\), calculez la médiane des pentes partielles formées avec tous les autres points : \(\text{med}_j \left( \frac{y_j - y_i}{x_j - x_i} \right)\) pour \(j \neq i\). Cela produit \(n\) médianes partielles (une par point \(i\)).
 
 2. La pente estimée \(\hat{\beta}\) est alors la médiane de ces \(n\) médianes partielles : \(\hat{\beta} = \text{med}_i \left( \text{med}_j \frac{y_j - y_i}{x_j - x_i} \right)\).
 
 L'ordonnée à l'origine \(\hat{\alpha}\) est estimée de manière analogue : pour chaque point \(i\), calculez la médiane des intercepts partielles \(y_j - \hat{\beta} x_j\) pour \(j \neq i\), puis prenez la médiane de ces \(n\) valeurs.
 
 Cette approche confère une haute robustesse car chaque médiane partielle est résistante aux valeurs aberrantes affectant un sous-ensemble des paires, et la médiane globale renforce cette propriété. Lorsque les erreurs sont normalement distribuées sans valeurs aberrantes, l'estimateur reste cohérent avec les moindres carrés ordinaires.
 
 \subsection{Déviation absolue médiane (MAD)}
 
 La déviation absolue médiane (MAD) est un estimateur robuste de l'échelle, introduit par \citep{hampel1974influence}, et défini pour un échantillon univarié $\{x_1, \dots, x_n\}$ comme suit :
 \[
 \mathrm{MAD}_n = b \cdot \mathrm{med}_{i=1}^n |x_i - \mathrm{med}_{j=1}^n x_j|,
 \]
 où $\mathrm{med}$ désigne la médiane et $b \approx 1,4826$ est une constante de correction assurant la consistance de l'estimateur avec l'écart-type $\sigma$ sous une distribution gaussienne.
 
 Cette constante $b$ découle des propriétés de la distribution normale. Pour une variable aléatoire $X \sim \mathcal{N}(\mu, \sigma^2)$, la médiane des écarts absolus à la médiane est égale à $\Phi^{-1}(3/4) \sigma$, où $\Phi$ est la fonction de répartition cumulative de la loi normale centrée réduite. Comme $\Phi^{-1}(0,75) \approx 0,6745$, on obtient $b = 1 / \Phi^{-1}(0,75) \approx 1,4826$. Ainsi, $\text{MAD}_n$ converge en espérance vers $\sigma$ lorsque $n \to \infty$.
 
 La MAD possède un point de rupture de 50 \% et une efficacité gaussienne de 37 \%, ce qui en fait un outil fondamental en statistique robuste, bien que des alternatives plus efficaces existent \citep{rousseeuw1993alternatives}.